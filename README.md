# StarGAN v2 

Имплементация статьи [StarGAN v2: Diverse Image Synthesis for Multiple Domains](https://arxiv.org/abs/1912.01865). 

### Abstract

Предложенная модель решает задачу image-to-image переноса стиля от одного изображения к другому. Эта статья является продолжением идеи предложенной в оригинальной статье [StarGAN](https://arxiv.org/pdf/1711.09020), где решается проблема масштабируемости CycleGAN на несколько доменов. Проблема CycleGAN была в том, что для $k$ доменов требуется $k(k-1)$ генераторов, которые в целом избыточны и выучивают одни и те же признаки. 

В StarGAN генератор принимает на вход не только картинку, но и метку домена, что позволяет переносить изображение из одного домена в любой другой используя один генератор. Однако так как метка домена это фиксированный one-hot вектор, то модель выучивает детерминированное отображение из-за чего страдает разнообразие (diversity) сгенерированных изображений, хотя потенциально изображения из одного домена могут быть в разных стилях. 

Главное нововведение StarGANv2 это использование стилевого вектора (style-code) вместо метки домена. Стилевой вектор может генерироваться из гауссовского шума (MappingNetwork), а может извлекаться из референсного изображения (StyleEncoder). В результате модель опередила своих конкурентов в image-to-image задаче по качеству переноса стиля и разнообразия сгенерированных изображений.

## Requirements
Все необходимые библиотеки можно скачать используя команду 
```shell
pip install -r requirements.txt
```

## Dataset
Авторы использовали датасеты CelebA-HQ со знаменитостями и AFHQ с лицами животных. В этой имплементации я попробую обучить модель на более простом датасете [CelebA](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) c 200k лицами знаменитостей и их атрибутами. \
Cкачать датасет можно так:

```python
from dataset import download_dataset

download_dataset("./data/celeba")
```

## Train

Все параметры обучения заданы в файле `config.py`. \
Для запуска процесса обучения надо вызвать команду 
```shell
python train.py
```


## Load pretrained

Обученную модель можно загрузить используя следующий код

```python
from config import build_args
from utils import load_model
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

args = build_args()
args.resume_iter = 40000

nets = load_model(args, device, save_path="./checkpoints")
```

## Inference

Есть 2 способа гененрировать картинку в тагрет домене: используя стиль референсной картинки или используя сгенерированный стиль из гауссовского шума

1) Generate style from reference image

```python
from utils import generate_image

x_fake = generate_image(nets, x_real, y_trg, x_ref)
```

2) Generate style from latent code

```python
x_fake = generate_image(nets, x_real, y_trg)
```

## Report

Обучение длилось 40к итераций (1.5 эпохи селебы) на V100 это порядка 24 часов. Я обучал не до сходимости, так как гпушки не бесплатные. Если обучать дольше, то качество генерации еще вырастет. Все четыре сети были на Adam оптимизаторах с параметрами из статьи. \
Архитектура генератора представляет из себя энкодер-декодер структуру с использованием AdaIN в декодере вместо обычной Instance нормализации, которая как раз переносит стиль на фото. Дискриминатор это сверточный бинарный классификатор. Mapping Network это полносвязная сеть с отдельной головой на каждый домен, которая выдают вектор стиля на выходе. Style Encoder это сверточная сеть из резблоков тоже с отдельной головой на каждый домен. \
Из аугментаций я использовал случайный кроп и отзеркаливание.

[Логи обучения на WandB](https://wandb.ai/zhan2/StarGANv2/runs/ugrs5frf?workspace=user-zhan2)

Примеры изображений, анализ модели и результаты обучения представлены в ноутбуке `stargan_report.ipynb`

## Possible improvements
Так как мы обучаем по сути модифицированный WassersteinGAN, то все проблемы ганов здесь имеют место, оптимизаторы можно поменять, например SGD для дискриминатора, как советуют в [GAN tricks](https://wangyi111.github.io/posts/2020/10/gan-tricks/). Вместо жестких лейблов можно использовать label smoothing, чтобы обучение было стабильнее. Также можно поэкспериментировать с дополнительными картиночными аугментациями.
